#include "{{plugin_name}}.h"
#include <cuda_runtime.h>
#include <thread>
#include <stdio.h>
#include <nvfunctional>
#include <chrono>

#define BLOCKSIZE_X 16
#define BLOCKSIZE_Y 16

using namespace nvinfer1;
using namespace plugin;

// CUDA Runtime error messages
#ifdef __DRIVER_TYPES_H__
static const char *_cudaGetErrorEnum(cudaError_t error)
{
  return cudaGetErrorName(error);
}
#endif

template <typename T>
void check(T result, char const *const func, const char *const file,
           int const line)
{
  if (result)
  {
    fprintf(stderr, "CUDA error at %s:%d code=%d(%s) \"%s\" \n", file, line,
            static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);
    exit(EXIT_FAILURE);
  }
}
#define checkCudaErrors(val) check((val), #val, __FILE__, __LINE__)

{% for case in cases -%}
{% if loop.first %}
{{case.plugin_template._plugin_kernels_body}}
{% else %}
{{case.plugin_template._plugin_kernels_body | rm_part_define}}
{% endif %}
{%- endfor %}
PluginFieldCollection {{plugin_name}}Creator::mFC{};
std::vector<PluginField> {{plugin_name}}Creator::mPluginAttributes;

int {{plugin_name}}::enqueue(const nvinfer1::PluginTensorDesc* inputDesc, const nvinfer1::PluginTensorDesc* outputDesc, const void* const* inputs, void* const* outputs, void* workspace, cudaStream_t stream) noexcept {
    {% for case in cases -%}
    {% if loop.first -%} 
    if( inputDesc[0].dims.d[0] == {{ case.batch_size }}){
      {% for constant in case.plugin_template._plugin_constant_init -%}
      const {{constant.type}} constant_{{constant.index}}[{{constant.length}}] = { {{constant.value}} };
      checkCudaErrors(cudaMemcpyAsync({{constant.pos}}, &constant_{{constant.index}}, {{constant.length}} * sizeof({{constant.type}}), cudaMemcpyHostToDevice, stream));
      {%- endfor %}
      dim3 dimBlock, dimGrid;
      {% for kernel in case.plugin_template._plugin_kernels_params %}
      dimGrid = dim3{{kernel.grid_dim}};
      dimBlock = dim3{{kernel.block_dim}};
      {{kernel.name}}<<<dimGrid, dimBlock, 0, stream>>>({{kernel.enqueue_params}});
      {% endfor %}
    }
    {% else -%}
    else if( {{ loop.previtem.batch_size }}  < inputDesc[0].dims.d[0] && inputDesc[0].dims.d[0] <= {{ case.batch_size }}){
      int bs = inputDesc[{{plugin_tensor_input_index[0]}}].dims.d[0];
      int offset_input_0 = {{ case.plugin_template._plugin_workspace_size }};
      {%- for inputShape in case.dy_plugin_input_size_type_without_bs %}
      checkCudaErrors(cudaMemcpyAsync(workspace + offset_input_{{loop.index0}}, (void *)(inputs[{{plugin_tensor_input_index[loop.index0]}}]), bs * {{inputShape.size}} * sizeof({{ inputShape.dtype }}), cudaMemcpyDeviceToDevice));
      int offset_input_{{loop.index0}}_padding = offset_input_{{loop.index0}} + bs * {{inputShape.size}} * sizeof({{ inputShape.dtype }});
      checkCudaErrors(cudaMemcpyAsync(workspace + offset_input_{{loop.index0}}_padding, (void *)(inputs[{{plugin_tensor_input_index[loop.index0]}}]), ({{case.batch_size}} - bs) * {{inputShape.size}} * sizeof({{ inputShape.dtype }}), cudaMemcpyDeviceToDevice));
      {%- if loop.last %}
      int offset_output_0 = offset_input_{{loop.index0}} + {{case.batch_size}} * {{inputShape.size}} * sizeof({{inputShape.dtype}});;
      {% else %}
      int offset_input_{{loop.index}} = offset_input_{{loop.index0}} + {{case.batch_size}} * {{inputShape.size}} * sizeof({{inputShape.dtype}});
      {%- endif %}
      {%- endfor %}
      {%- for outputShape in case.dy_plugin_output_size_type_without_bs %}
      {%- if not loop.last %}
      int offset_output_{{loop.index}} = offset_output_{{loop.index0}} + {{case.batch_size}} * {{outputShape.size}} * sizeof({{outputShape.dtype}});
      {%- endif %}
      {%- endfor %}
      {% for constant in case.plugin_template._plugin_constant_init -%}
      const {{constant.type}} constant_{{constant.index}}[{{constant.length}}] = { {{constant.value}} };
      checkCudaErrors(cudaMemcpyAsync({{constant.pos}}, &constant_{{constant.index}}, {{constant.length}} * sizeof({{constant.type}}), cudaMemcpyHostToDevice, stream));
      {%- endfor -%}
      dim3 dimBlock, dimGrid;
      {%- for kernel in case.plugin_template._plugin_kernels_params %}
      dimGrid = dim3{{kernel.grid_dim}};
      dimBlock = dim3{{kernel.block_dim}};
      {{kernel.name}}<<<dimGrid, dimBlock, 0, stream>>>({{kernel.enqueue_params}});
      {%- endfor %}
      {%- for outputShape in case.dy_plugin_output_size_type_without_bs %}
      checkCudaErrors(cudaMemcpyAsync((void *)(outputs[{{loop.index0}}]), (workspace + offset_output_{{loop.index0}}), bs * {{outputShape.size}} * sizeof({{ outputShape.dtype }}), cudaMemcpyDeviceToDevice));
      {%- endfor %}
    }
    {%- endif %}
    {%- endfor %}
    return 0;
}

REGISTER_TENSORRT_PLUGIN({{plugin_name}}Creator);
